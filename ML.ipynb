{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23ef4db",
   "metadata": {},
   "source": [
    "# Import and preprocess of data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93413e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#dataset_path = experiment_config[\"design_state_data\"][\"session_info\"][\"dataset\"]\n",
    "df = pd.read_csv(r'C:/Users/psara/Company/Dentrite/DS_Assignment - internship/Screening Test - DS/iris.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "247ed792",
   "metadata": {},
   "source": [
    "Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a602f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebdfd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of data points for each class in the 'species' column\n",
    "class_counts = df['species'].value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e97a8e7c",
   "metadata": {},
   "source": [
    "Convert species column into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f69306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the unique entries in the 'species' column\n",
    "unique_species = df['species'].unique()\n",
    "\n",
    "# Print the unique species entries\n",
    "print(unique_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a479987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13b8029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from species to integer values\n",
    "species_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "\n",
    "# Apply the mapping to the 'species' column\n",
    "df['species'] = df['species'].map(species_mapping)\n",
    "\n",
    "# Print the first few rows of the DataFrame with converted species column\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19621d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 6.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ab7d7",
   "metadata": {},
   "source": [
    "# Imputation of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3104c445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "species         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of missing values in each column\n",
    "df.isnull().sum()\n",
    "# No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3c8b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0             5.1          3.5           1.4          0.2        0\n",
      "1             4.9          3.0           1.4          0.2        0\n",
      "2             4.7          3.2           1.3          0.2        0\n",
      "3             4.6          3.1           1.5          0.2        0\n",
      "4             5.0          3.6           1.4          0.2        0\n",
      "..            ...          ...           ...          ...      ...\n",
      "145           6.7          3.0           5.2          2.3        2\n",
      "146           6.3          2.5           5.0          1.9        2\n",
      "147           6.5          3.0           5.2          2.0        2\n",
      "148           6.2          3.4           5.4          2.3        2\n",
      "149           5.9          3.0           5.1          1.8        2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing_values(data, feature_details):\n",
    "    impute_method = feature_details[\"feature_details\"][\"missing_values\"]\n",
    "    impute_value = feature_details[\"feature_details\"][\"impute_value\"]\n",
    "\n",
    "    if impute_method == \"Impute\":\n",
    "        impute_with = feature_details[\"feature_details\"][\"impute_with\"]\n",
    "        if impute_with == \"Average of values\":\n",
    "            imputer = SimpleImputer(strategy=\"mean\")\n",
    "        elif impute_with == \"custom\":\n",
    "            imputer = SimpleImputer(strategy=\"constant\", fill_value=impute_value)\n",
    "        else:\n",
    "            return data  # No imputation needed\n",
    "\n",
    "        data[feature_details[\"feature_name\"]] = imputer.fit_transform(data[[feature_details[\"feature_name\"]]])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the JSON configuration\n",
    "json_file_path = r\"C:\\Users\\psara\\Company\\Dentrite\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui_modified.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    experiment_config = json.load(json_file)\n",
    "\n",
    "# Iterate through each feature in the configuration\n",
    "for feature_name, feature_details in experiment_config[\"design_state_data\"][\"feature_handling\"].items():\n",
    "    if feature_details.get(\"feature_details\", {}).get(\"missing_values\") == \"Impute\":\n",
    "        df = impute_missing_values(df, feature_details)\n",
    "\n",
    "# Print the DataFrame after imputation\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50464384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract all columns except the \"species\" column\n",
    "features = [col for col in df.columns if col != \"species\"]\n",
    "\n",
    "# Create a new DataFrame containing only the selected features\n",
    "features_df = df[features]\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9b3fc",
   "metadata": {},
   "source": [
    "# Feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b75f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reduction needed.\n",
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the JSON configuration\n",
    "json_file_path = r\"C:\\Users\\psara\\Company\\Dentrite\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui_modified.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    experiment_config = json.load(json_file)\n",
    "\n",
    "# Display the loaded experiment configuration\n",
    "#print(json.dumps(experiment_config, indent=4))\n",
    "\n",
    "# Apply feature reduction\n",
    "feature_reduction_details = experiment_config[\"design_state_data\"][\"feature_reduction\"]\n",
    "reduction_method = feature_reduction_details[\"feature_reduction_method\"]\n",
    "\n",
    "\n",
    "\n",
    "if reduction_method == \"No Reduction\":\n",
    "    df_reduced = features_df\n",
    "    print(\"No reduction needed.\")  # No reduction needed\n",
    "\n",
    "    \n",
    "\n",
    "elif reduction_method == \"Correlation with target\":\n",
    "    num_of_features_to_keep = int(feature_reduction_details[\"Correlation with target\"][\"num_of_features_to_keep\"])\n",
    "    target_column = experiment_config[\"design_state_data\"][\"target\"][\"target\"]\n",
    "    features = [col for col in df.columns if col != target_column]\n",
    "        \n",
    "    corr_matrix = df[features + [target_column]].corr()\n",
    "    corr_with_target = corr_matrix[target_column].abs().sort_values(ascending=False)\n",
    "    selected_features = corr_with_target.head(num_of_features_to_keep + 1).index.tolist()\n",
    "        \n",
    "    df_reduced = df[selected_features]\n",
    "    print(\"Reduced DataFrame Shape:\", df_reduced.shape)\n",
    "\n",
    "elif reduction_method == \"Tree-based\":\n",
    "    print(\"tree\")\n",
    "    num_of_features_to_keep = int(feature_reduction_details[\"Tree-based\"][\"num_of_features_to_keep\"])\n",
    "    num_of_trees = int(feature_reduction_details[\"Tree-based\"][\"num_of_trees\"])\n",
    "    depth_of_trees = int(feature_reduction_details[\"Tree-based\"][\"depth_of_trees\"])\n",
    "\n",
    "    target_column = experiment_config[\"design_state_data\"][\"target\"][\"target\"]\n",
    "    features = [col for col in df.columns if col != target_column]\n",
    "        \n",
    "    X = df[features]\n",
    "    y = df[target_column]\n",
    "        \n",
    "    rf = RandomForestRegressor(n_estimators=num_of_trees, max_depth=depth_of_trees)\n",
    "    rf.fit(X, y)\n",
    "        \n",
    "    importance = rf.feature_importances_\n",
    "    feature_importance = list(zip(features, importance))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    selected_features = [feat for feat, _ in feature_importance[:num_of_features_to_keep]]\n",
    "        \n",
    "    df_reduced = df[selected_features + [target_column]]\n",
    "    print(\"Reduced DataFrame Shape:\", df_reduced.shape)\n",
    "\n",
    "\n",
    "elif reduction_method == \"PCA\":\n",
    "    num_of_features_to_keep = int(feature_reduction_details[\"Principal Component Analysis\"][\"num_of_features_to_keep\"])\n",
    "        \n",
    "    target_column = experiment_config[\"design_state_data\"][\"target\"][\"target\"]\n",
    "    features = [col for col in df.columns if col != target_column]\n",
    "        \n",
    "    X = data[features]\n",
    "    pca = PCA(n_components=num_of_features_to_keep)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "        \n",
    "    reduced_feature_names = [f\"PC{i+1}\" for i in range(num_of_features_to_keep)]\n",
    "    reduced_data = pd.DataFrame(X_reduced, columns=reduced_feature_names)\n",
    "        \n",
    "    #return pd.concat([reduced_data, data[target_column]], axis=1)\n",
    "    df_reduced = pd.concat([reduced_data, features_df[\"target\"]], axis=1)\n",
    "    print(\"Reduced DataFrame Shape:\", df_reduced.shape)\n",
    "\n",
    "    \n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc31b71",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf5863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import json\n",
    "\n",
    "# Load the JSON configuration\n",
    "json_file_path = r\"C:\\Users\\psara\\Company\\Dentrite\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui_modified.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Get the algorithms from the JSON\n",
    "algorithms = config[\"design_state_data\"][\"algorithms\"]\n",
    "\n",
    "# Initialize the model objects\n",
    "model_objects = []\n",
    "\n",
    "for algo_name, algo_params in algorithms.items():\n",
    "    prediction_type = algo_params.get(\"prediction_type\")\n",
    "    \n",
    "    if prediction_type == \"Classification\":\n",
    "        if algo_name == \"RandomForestClassifier\":\n",
    "            # ... Create RandomForestClassifier object with specified parameters\n",
    "            rf_params = {\n",
    "                \"n_estimators\": algo_params[\"max_trees\"],\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"min_samples_leaf\": algo_params[\"min_samples_per_leaf_max_value\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            rf_classifier = RandomForestClassifier(**rf_params)\n",
    "            model_objects.append(rf_classifier)\n",
    "        elif algo_name == \"GBTClassifier\":\n",
    "            # ... Create GradientBoostingClassifier object with specified parameters\n",
    "            gb_params = {\n",
    "                \"n_estimators\": algo_params[\"num_of_BoostingStages\"],\n",
    "                \"learning_rate\": algo_params[\"learningRate\"],\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"subsample\": algo_params[\"max_subsample\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            gb_classifier = GradientBoostingClassifier(**gb_params)\n",
    "            model_objects.append(gb_classifier)\n",
    "        elif algo_name == \"LogisticRegression\":\n",
    "            # ... Create LogisticRegression object with specified parameters\n",
    "            logistic_params = {\n",
    "                \"penalty\": \"l2\",\n",
    "                \"solver\": \"lbfgs\",\n",
    "                \"max_iter\": algo_params[\"max_iter\"],  # Provide the integer value directly\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            logistic_classifier = LogisticRegression(**logistic_params)\n",
    "            model_objects.append(logistic_classifier)\n",
    "        elif algo_name == \"xg_boost\":\n",
    "            xgb_params = {\n",
    "            # Fill in the parameters based on the JSON values\n",
    "            }\n",
    "            xgb_model = XGBClassifier(**xgb_params) if aXGBClassifierlgo_params[\"use_gradient_boosted_tree\"] else XGBRegressor(**xgb_params)\n",
    "            model_objects.append(xgb_model)    \n",
    "        elif algo_name == \"DecisionTreeClassifier\":\n",
    "            dt_params = {\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"min_samples_leaf\": algo_params[\"min_samples_per_leaf\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            dt_classifier = DecisionTreeClassifier(**dt_params)\n",
    "            model_objects.append(dt_classifier)\n",
    "        elif algo_name == \"SVM\":\n",
    "            svm_params = {\n",
    "                \"C\": algo_params[\"c_value\"],\n",
    "                \"kernel\": \"rbf\",\n",
    "                \"gamma\": \"scale\"  # You might need to adjust this parameter\n",
    "            }\n",
    "            svm_classifier = SVC(**svm_params)\n",
    "            model_objects.append(svm_classifier)\n",
    "        elif algo_name == \"KNN\":\n",
    "            knn_params = {\n",
    "            \"n_neighbors\": algo_params[\"k_value\"],\n",
    "            \"weights\": \"distance\" if algo_params[\"distance_weighting\"] else \"uniform\",\n",
    "            \"algorithm\": algo_params[\"neighbour_finding_algorithm\"],\n",
    "            \"p\": algo_params[\"p_value\"]\n",
    "            }\n",
    "            knn_classifier = KNeighborsClassifier(**knn_params)\n",
    "            model_objects.append(knn_classifier)\n",
    "        # Add conditions for other classification models\n",
    "        \n",
    "    elif prediction_type == \"Regression\":\n",
    "        if algo_name == \"RandomForestRegressor\":\n",
    "            # ... Create RandomForestRegressor object with specified parameters\n",
    "            rf_params = {\n",
    "                \"n_estimators\": algo_params[\"max_trees\"],\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"min_samples_leaf\": algo_params[\"min_samples_per_leaf_max_value\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            rf_regressor = RandomForestRegressor(**rf_params)\n",
    "            model_objects.append(rf_regressor)\n",
    "        elif algo_name == \"GBTRegressor\":\n",
    "            # ... Create GradientBoostingRegressor object with specified parameters\n",
    "            gb_params = {\n",
    "                \"n_estimators\": algo_params[\"num_of_BoostingStages\"],\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"subsample\": algo_params[\"max_subsample\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            gb_regressor = GradientBoostingRegressor(**gb_params)\n",
    "            model_objects.append(gb_regressor)\n",
    "        elif algo_name == \"LinearRegression\":\n",
    "            # ... Create LinearRegression object with specified parameters\n",
    "            linear_params = {\n",
    "                \"fit_intercept\": True\n",
    "            }\n",
    "            linear_regressor = LinearRegression(**linear_params)\n",
    "            model_objects.append(linear_regressor)\n",
    "        elif algo_name == \"LassoRegression\":\n",
    "            lasso_params = {\n",
    "            \"alpha\": (algo_params[\"min_regparam\"] + algo_params[\"max_regparam\"]) / 2,\n",
    "            \"max_iter\": algo_params[\"max_iter\"],\n",
    "            \"random_state\": None\n",
    "            }\n",
    "            lasso_regressor = Lasso(**lasso_params)\n",
    "            model_objects.append(lasso_regressor)\n",
    "        elif algo_name == \"RidgeRegression\":\n",
    "            ridge_params = {\n",
    "            \"alpha\": (algo_params[\"min_regparam\"] + algo_params[\"max_regparam\"]) / 2,\n",
    "            \"max_iter\": algo_params[\"max_iter\"],\n",
    "            \"random_state\": None\n",
    "            }\n",
    "            ridge_regressor = Ridge(**ridge_params)\n",
    "            model_objects.append(ridge_regressor)            \n",
    "        elif algo_name == \"ElasticNetRegression\":\n",
    "            elastic_net_params = {\n",
    "            \"alpha\": (algo_params[\"min_regparam\"] + algo_params[\"max_regparam\"]) / 2,\n",
    "            \"l1_ratio\": (algo_params[\"min_elasticnet\"] + algo_params[\"max_elasticnet\"]) / 2,\n",
    "            \"max_iter\": algo_params[\"max_iter\"],\n",
    "            \"random_state\": None\n",
    "            }\n",
    "            elastic_net_regressor = ElasticNet(**elastic_net_params)\n",
    "            model_objects.append(elastic_net_regressor)\n",
    "        elif algo_name == \"DecisionTreeRegressor\":\n",
    "            dt_params = {\n",
    "                \"max_depth\": algo_params[\"max_depth\"],\n",
    "                \"min_samples_leaf\": algo_params[\"min_samples_per_leaf\"],\n",
    "                \"random_state\": None\n",
    "            }\n",
    "            dt_regressor = DecisionTreeRegressor(**dt_params)\n",
    "            model_objects.append(dt_regressor)\n",
    "        elif algo_name == \"SGD\":\n",
    "            sgd_params = {\n",
    "            \"loss\": \"log\" if algo_params[\"use_logistics\"] else \"modified_huber\" if algo_params[\"use_modified_hubber_loss\"] else \"hinge\",\n",
    "            \"penalty\": \"l1\" if algo_params[\"use_l1_regularization\"] else \"l2\" if algo_params[\"use_l2_regularization\"] else \"elasticnet\",\n",
    "            \"alpha\": algo_params[\"alpha_value\"],\n",
    "            \"max_iter\": algo_params[\"max_iterations\"] if algo_params[\"max_iterations\"] else 1000,\n",
    "            \"tol\": algo_params[\"tolerance\"],\n",
    "            \"l1_ratio\": 0.15,  # You might want to adjust this\n",
    "            \"random_state\": None\n",
    "            }\n",
    "            sgd_regressor = SGDRegressor(**sgd_params) if algo_params[\"is_selected\"] else None\n",
    "            model_objects.append(sgd_regressor)\n",
    "        elif algo_name == \"extra_random_trees\":\n",
    "            extra_trees_params = {\n",
    "            \"n_estimators\": algo_params[\"num_of_trees\"],\n",
    "            \"max_depth\": algo_params[\"max_depth\"],\n",
    "            \"min_samples_leaf\": algo_params[\"min_samples_per_leaf\"],\n",
    "            \"n_jobs\": None\n",
    "            }\n",
    "            extra_trees_regressor = ExtraTreesRegressor(**extra_trees_params)\n",
    "            model_objects.append(extra_trees_regressor)\n",
    "        elif algo_name == \"neural_network\":\n",
    "            nn_params = {\n",
    "            \"hidden_layer_sizes\": tuple(algo_params[\"hidden_layer_sizes\"]),\n",
    "            \"activation\": algo_params[\"activation\"],\n",
    "            \"alpha\": algo_params[\"alpha_value\"],\n",
    "            \"max_iter\": algo_params[\"max_iterations\"],\n",
    "            \"tol\": algo_params[\"convergence_tolerance\"],\n",
    "            \"early_stopping\": algo_params[\"early_stopping\"],\n",
    "            \"solver\": algo_params[\"solver\"],\n",
    "            \"shuffle\": algo_params[\"shuffle_data\"],\n",
    "            \"learning_rate_init\": algo_params[\"initial_learning_rate\"],\n",
    "            \"batch_size\": \"auto\" if algo_params[\"automatic_batching\"] else None,\n",
    "            \"beta_1\": algo_params[\"beta_1\"],\n",
    "            \"beta_2\": algo_params[\"beta_2\"],\n",
    "            \"epsilon\": algo_params[\"epsilon\"],\n",
    "            \"power_t\": algo_params[\"power_t\"],\n",
    "            \"momentum\": algo_params[\"momentum\"],\n",
    "            \"nesterovs_momentum\": algo_params[\"use_nesterov_momentum\"]\n",
    "            }\n",
    "            nn_regressor = MLPRegressor(**nn_params) if algo_params[\"is_selected\"] else None\n",
    "            model_objects.append(nn_regressor)\n",
    "        # Add conditions for other regression models\n",
    "# Now you can use these model objects for training and prediction\n",
    "for model in model_objects:\n",
    "    print(model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b03db",
   "metadata": {},
   "source": [
    "# Model prediction with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4368a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: RandomForestRegressor\n",
      "Mean Squared Error: 0.03584041525415714\n",
      "R-squared: 0.92534802071619\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: GBTRegressor\n",
      "Mean Squared Error: 0.03084022076873657\n",
      "R-squared: 0.935762922789551\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: LinearRegression\n",
      "Mean Squared Error: 0.037783086103526056\n",
      "R-squared: 0.9213016327774921\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: RidgeRegression\n",
      "Mean Squared Error: 0.03853924217385969\n",
      "R-squared: 0.9197266357553433\n",
      "Best Parameters: {'max_iter': 50}\n",
      "\n",
      "\n",
      "Algorithm: LassoRegression\n",
      "Mean Squared Error: 0.30154854796287334\n",
      "R-squared: 0.37190471159576466\n",
      "Best Parameters: {'max_iter': 50}\n",
      "\n",
      "\n",
      "Algorithm: ElasticNetRegression\n",
      "Mean Squared Error: 0.1415225342753024\n",
      "R-squared: 0.7052227988433609\n",
      "Best Parameters: {'max_iter': 50}\n",
      "\n",
      "\n",
      "Algorithm: DecisionTreeRegressor\n",
      "Mean Squared Error: 0.07766666666666668\n",
      "R-squared: 0.8382281469138373\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: SGD\n",
      "Mean Squared Error: 0.05414047044479643\n",
      "R-squared: 0.8872308468135879\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: extra_random_trees\n",
      "Mean Squared Error: 0.039641474999999975\n",
      "R-squared: 0.9174307956675692\n",
      "Best Parameters: {}\n",
      "\n",
      "\n",
      "Algorithm: neural_network\n",
      "Mean Squared Error: 0.05154232733371235\n",
      "R-squared: 0.8926425175302805\n",
      "Best Parameters: {'beta_1': 0, 'beta_2': 0, 'early_stopping': True, 'hidden_layer_sizes': [67, 89], 'momentum': 0, 'power_t': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load JSON from file (replace with your file path)\n",
    "file_path = r\"C:\\Users\\psara\\Company\\Dentrite\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui_modified.json\"\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    design_state_data = json.load(json_file)\n",
    "\n",
    "# Load your dataset (replace with loading your dataset)\n",
    "# For example: df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Iterate through algorithms\n",
    "for algo_name, algo_params in design_state_data[\"design_state_data\"][\"algorithms\"].items():\n",
    "    if algo_params[\"is_selected\"]:\n",
    "        if design_state_data[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"Regression\":\n",
    "            label = design_state_data[\"design_state_data\"][\"target\"][\"target\"]\n",
    "            features = df.drop(label, axis=1)\n",
    "            target = df[label]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
    "            \n",
    "            if \"RandomForestRegressor\" in algo_name:\n",
    "                model = RandomForestRegressor()\n",
    "            elif \"GBTRegressor\" in algo_name:\n",
    "                model = GradientBoostingRegressor()\n",
    "            elif \"LinearRegression\" in algo_name:\n",
    "                model = LinearRegression()\n",
    "            elif \"LassoRegression\" in algo_name:\n",
    "                model = Lasso()\n",
    "            elif \"RidgeRegression\" in algo_name:\n",
    "                model = Ridge()\n",
    "            elif \"ElasticNetRegression\" in algo_name:\n",
    "                model = ElasticNet()   \n",
    "            elif \"DecisionTreeRegressor\" in algo_name:\n",
    "                model = DecisionTreeRegressor() \n",
    "            elif \"SGD\" in algo_name:\n",
    "                model = SGDRegressor()   \n",
    "            elif \"extra_random_trees\" in algo_name:\n",
    "                model = ExtraTreesRegressor()                \n",
    "            elif \"neural_network\" in algo_name:\n",
    "                model = MLPRegressor()        \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            param_grid = {key: [value] for key, value in algo_params.items() if key in model.get_params()}\n",
    "            \n",
    "            # Parameters to remove\n",
    "            params_to_remove = ['feature_sampling_statergy', 'is_selected', 'max_trees', \n",
    "                                'min_depth', 'min_samples_per_leaf_min_value', \n",
    "                                'min_samples_per_leaf_max_value', 'min_trees', \n",
    "                                'model_name', 'parallelism', 'fixed_number','max_depth','activation','epsilon','solver']\n",
    "            \n",
    "            for param in params_to_remove:\n",
    "                param_grid.pop(param, None)\n",
    "            \n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = grid_search.predict(X_test)\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            print(f\"Algorithm: {algo_name}\")\n",
    "            print(f\"Mean Squared Error: {mse}\")\n",
    "            print(f\"R-squared: {r2}\")\n",
    "            print(\"Best Parameters:\", grid_search.best_params_)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        if design_state_data[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"Classification\":\n",
    "            features = df.drop(\"species\", axis=1)\n",
    "            target = df[\"species\"]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
    "            \n",
    "            if \"RandomForestClassifier\" in algo_name:\n",
    "                model = RandomForestClassifier()\n",
    "            elif \"GBTClassifier\" in algo_name:\n",
    "                model = GradientBoostingClassifier()                \n",
    "            elif \"LogisticRegression\" in algo_name:\n",
    "                model = LogisticRegression(solver='liblinear')      \n",
    "            elif \"xg_boost\" in algo_name:\n",
    "                model = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, early_stopping_rounds=2)  # Set early_stopping_rounds here\n",
    "                eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "            elif \"SVM\" in algo_name:\n",
    "                model = SVC(probability=True)  \n",
    "            elif \"KNN\" in algo_name:\n",
    "                model = KNeighborsClassifier()                \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            param_grid = {key: [value] for key, value in algo_params.items() if key in model.get_params()}\n",
    "            # Parameters to remove\n",
    "            params_to_remove = ['feature_sampling_statergy', 'is_selected', 'max_trees', \n",
    "                    'min_depth', 'min_samples_per_leaf_min_value', \n",
    "                    'min_samples_per_leaf_max_value', 'min_trees', \n",
    "                    'model_name', 'parallelism', 'fixed_number','max_depth','activation','epsilon','solver',\n",
    "                    'min_child_weight','max_depth_of_tree', 'learningRate', 'l1_regularization', 'l2_regularization', \n",
    "                    'gamma','sub_sample','col_sample_by_tree']\n",
    "            for param in params_to_remove:\n",
    "                param_grid.pop(param, None)\n",
    "\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "            if \"xg_boost\" in algo_name:\n",
    "                grid_search.fit(X_train, y_train, eval_set=eval_set, verbose=10)\n",
    "            else:\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "           # Predict the class labels\n",
    "            pred = grid_search.predict(X_test)\n",
    "\n",
    "            # Get prediction probabilities\n",
    "            if hasattr(grid_search.best_estimator_, \"predict_proba\"):\n",
    "                pred_prob = grid_search.predict_proba(X_test)\n",
    "            else:\n",
    "             # Use decision_function() for SVM since it doesn't support predict_proba()\n",
    "                pred_prob = grid_search.decision_function(X_test)\n",
    "\n",
    "            # Calculate AUC\n",
    "            if \"SGD\" in algo_name:\n",
    "            # For SGDClassifier, calculate ROC AUC using decision values\n",
    "                auc_value = roc_auc_score(y_test, pred_prob)\n",
    "            else:\n",
    "            # For other classifiers, calculate ROC AUC using predicted probabilities\n",
    "                auc_value = roc_auc_score(y_test, pred_prob, multi_class='ovr')\n",
    "   \n",
    "            \n",
    "\n",
    "            # Calculate F1 Score\n",
    "            f1 = f1_score(y_test, pred, average='micro')\n",
    "        \n",
    "            # Calculate threshold-based predicted labels for Logistic Regression\n",
    "            if \"LogisticRegression\" in algo_name:\n",
    "                y_pred_labels = (pred_prob[:, 1] >= 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred_labels = (pred_prob >= 0.5).astype(int)\n",
    "\n",
    "            # Calculate Confusion Matrix\n",
    "            # Calculate Confusion Matrix\n",
    "            conf_matrix = confusion_matrix(y_test, pred, labels=np.unique(y_test))\n",
    "\n",
    "\n",
    "            # Calculate Total Cost\n",
    "            cost_matrix_gain_true_true = design_state_data[\"design_state_data\"][\"metrics\"][\"cost_matrix_gain_for_true_prediction_true_result\"]\n",
    "            cost_matrix_gain_true_false = design_state_data[\"design_state_data\"][\"metrics\"][\"cost_matrix_gain_for_true_prediction_false_result\"]\n",
    "            cost_matrix_gain_false_true = design_state_data[\"design_state_data\"][\"metrics\"][\"cost_matrix_gain_for_false_prediction_true_result\"]\n",
    "            cost_matrix_gain_false_false = design_state_data[\"design_state_data\"][\"metrics\"][\"cost_matrix_gain_for_false_prediction_false_result\"]\n",
    "\n",
    "            total_cost = (conf_matrix[0, 0] * cost_matrix_gain_true_true +\n",
    "              conf_matrix[0, 1] * cost_matrix_gain_true_false +\n",
    "              conf_matrix[1, 0] * cost_matrix_gain_false_true +\n",
    "              conf_matrix[1, 1] * cost_matrix_gain_false_false)\n",
    "\n",
    "            # Print Results\n",
    "            print(f\"Algorithm: {algo_name}\")\n",
    "            print(f\"AUC: {auc_value}\")\n",
    "            print(f\"F1 Score: {f1}\")\n",
    "            print(f\"Total Cost: {total_cost}\")\n",
    "            print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
